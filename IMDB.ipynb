{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.375120Z",
     "start_time": "2020-11-23T10:53:52.309395Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.833499Z",
     "start_time": "2020-11-23T10:53:54.377802Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv('IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.842385Z",
     "start_time": "2020-11-23T10:53:54.834968Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_data['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.846518Z",
     "start_time": "2020-11-23T10:53:54.843674Z"
    }
   },
   "outputs": [],
   "source": [
    "review = imdb_data['review'].loc[1]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROECESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.851068Z",
     "start_time": "2020-11-23T10:53:54.847591Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "print(review)\n",
    "print('---------------------')\n",
    "review = remove_between_square_brackets(review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.855972Z",
     "start_time": "2020-11-23T10:53:54.852250Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "print(review)\n",
    "print('---------------------')\n",
    "review = strip_html(review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.860873Z",
     "start_time": "2020-11-23T10:53:54.857195Z"
    }
   },
   "outputs": [],
   "source": [
    "#clean the review\n",
    "def remove_special_characters(text, with_lower=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    if with_lower is False:\n",
    "        return text\n",
    "    return text.lower()\n",
    "\n",
    "print(review)\n",
    "print('---------------------')\n",
    "review = remove_special_characters(review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.873701Z",
     "start_time": "2020-11-23T10:53:54.862586Z"
    }
   },
   "outputs": [],
   "source": [
    "#split the review\n",
    "def split_review(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "tokenized_review = split_review(review)\n",
    "tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.880004Z",
     "start_time": "2020-11-23T10:53:54.875002Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:54.887159Z",
     "start_time": "2020-11-23T10:53:54.881058Z"
    }
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def porter_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    review_s = [ps.stem(word) for word in text]\n",
    "    return review_s\n",
    "\n",
    "tokenized_review_stemmed = porter_stemmer(tokenized_review)\n",
    "print(' '.join(tokenized_review))\n",
    "print('---------------------')\n",
    "print(' '.join(tokenized_review_stemmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:56.196740Z",
     "start_time": "2020-11-23T10:53:54.888320Z"
    }
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def wordnet_lemmatizer(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    review_l = [lem.lemmatize(word) for word in text]\n",
    "    return review_l\n",
    "\n",
    "tokenized_review_lemmatized = wordnet_lemmatizer(tokenized_review)\n",
    "print(' '.join(tokenized_review))\n",
    "print('---------------------')\n",
    "print(' '.join(tokenized_review_lemmatized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:56.201882Z",
     "start_time": "2020-11-23T10:53:56.198187Z"
    }
   },
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:56.206420Z",
     "start_time": "2020-11-23T10:53:56.202890Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop):\n",
    "    review_s = [word for word in text if word not in stop]\n",
    "    return review_s\n",
    "\n",
    "print(' '.join(tokenized_review_lemmatized))\n",
    "print('---------------------')\n",
    "tokenized_review_lemmatized = remove_stop_words(tokenized_review_lemmatized, stop)\n",
    "print(' '.join(tokenized_review_lemmatized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before  and after preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:53:56.210052Z",
     "start_time": "2020-11-23T10:53:56.207374Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(imdb_data['review'].loc[1])\n",
    "print('---------------------')\n",
    "print(' '.join(tokenized_review_lemmatized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T09:15:36.456976Z",
     "start_time": "2020-11-23T09:15:36.453971Z"
    }
   },
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:54:40.759457Z",
     "start_time": "2020-11-23T10:53:56.211052Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(imdb_data['review'].shape[0]):    \n",
    "    if i % 100 == 0:\n",
    "        print('At step', i)\n",
    "    review = imdb_data['review'].iloc[i]\n",
    "    review = remove_between_square_brackets(review)\n",
    "    review = strip_html(review)\n",
    "    review = remove_special_characters(review)\n",
    "    tokenized_review = split_review(review)\n",
    "    #tokenized_review_lemmatized = wordnet_lemmatizer(tokenized_review)\n",
    "    tokenized_review = remove_stop_words(tokenized_review, stop)\n",
    "    X.append(' '.join(tokenized_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:54:40.764129Z",
     "start_time": "2020-11-23T10:54:40.760882Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check difference\n",
    "index = 100\n",
    "\n",
    "print( imdb_data['review'].iloc[index])\n",
    "print('------------------')\n",
    "print(X[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:54:43.208765Z",
     "start_time": "2020-11-23T10:54:40.765254Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, imdb_data['sentiment'].values, test_size=0.2, random_state=42)\n",
    "print(\"Train shapes : X = {}, y = {}\".format(X_train.shape,y_train.shape))\n",
    "print(\"Test shapes : X = {}, y = {}\".format(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:54:43.219091Z",
     "start_time": "2020-11-23T10:54:43.210089Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:08.495633Z",
     "start_time": "2020-11-23T10:54:43.220159Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.fit_transform(X_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:46.992559Z",
     "start_time": "2020-11-23T10:55:08.497387Z"
    }
   },
   "outputs": [],
   "source": [
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "lr_tfidf=lr.fit(tfidf_vec_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:47.013095Z",
     "start_time": "2020-11-23T10:55:46.994293Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_tfidf_predict=lr.predict(tfidf_vec_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:47.031991Z",
     "start_time": "2020-11-23T10:55:47.014260Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test, lr_tfidf_predict,target_names=['Negative','Positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:59.816203Z",
     "start_time": "2020-11-23T10:55:47.034021Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "train_pad_sequence = pad_sequences(training_sequence,maxlen = 200)\n",
    "test_pad_sequence = pad_sequences(testing_sequence,maxlen = 200)\n",
    "print('Total Unique Words : {}'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:55:59.821058Z",
     "start_time": "2020-11-23T10:55:59.817437Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[1]\n",
    "print('----------')\n",
    "train_pad_sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:56:02.221118Z",
     "start_time": "2020-11-23T10:55:59.822037Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(10001 + 1,64 ,input_length=200,\n",
    "                            trainable=True),\n",
    "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(128,activation = 'relu',),\n",
    "                             tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(1,activation = tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T10:56:02.227960Z",
     "start_time": "2020-11-23T10:56:02.222579Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T11:09:57.800101Z",
     "start_time": "2020-11-23T10:56:02.229222Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy() , optimizer='Adam',  metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_pad_sequence,y_train,epochs = 30 ,validation_data=(test_pad_sequence,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_gpu",
   "language": "python",
   "name": "text_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
